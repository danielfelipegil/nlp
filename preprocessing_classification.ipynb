{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the twitter samples corpus from nltk and compute avg length of tweets in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading twitter_samples: <urlopen error [Errno 8]\n",
      "[nltk_data]     nodename nor servname provided, or not known>\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mtwitter_samples\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('twitter_samples')\n  \u001b[0m\n  Searched in:\n    - '/Users/danielgil/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/Users/danielgil/anaconda/nltk_data'\n    - '/Users/danielgil/anaconda/share/nltk_data'\n    - '/Users/danielgil/anaconda/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    674\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mtwitter_samples\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('twitter_samples')\n  \u001b[0m\n  Searched in:\n    - '/Users/danielgil/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/Users/danielgil/anaconda/nltk_data'\n    - '/Users/danielgil/anaconda/share/nltk_data'\n    - '/Users/danielgil/anaconda/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f614e04a1cbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'twitter_samples'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtwitter_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtwitter_samples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtweet_avg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mtwitter_samples\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('twitter_samples')\n  \u001b[0m\n  Searched in:\n    - '/Users/danielgil/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/Users/danielgil/anaconda/nltk_data'\n    - '/Users/danielgil/anaconda/share/nltk_data'\n    - '/Users/danielgil/anaconda/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('twitter_samples')\n",
    "from nltk.corpus import twitter_samples\n",
    "corpus=twitter_samples.strings()\n",
    "\n",
    "def tweet_avg(corpus):\n",
    "    avg=float(sum((map(len,corpus))))/float(len(corpus)) # use of maps instead of iterating\n",
    "    return avg\n",
    "\n",
    "def tweet_avg_iterate(corpus):\n",
    "    sum_len_tweets=0\n",
    "    for tweet in corpus:\n",
    "        sum_len_tweets+=len(tweet) #iterating over the corpus to get length of tweets\n",
    "\n",
    "    avg=float(sum_len_tweets)/float(len(corpus))\n",
    "    return avg\n",
    "        \n",
    "\n",
    "print(str(tweet_avg_iterate(corpus)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract hashtags using regular expressions. Let's consider all hashtags of length 8 or longer which consist only of lower case letters. In addition, the hashtag might occur at the beginning or the end of the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "reg_init=re.compile(r'^#([a-z]{8,})\\s') #boundaries are either space, the beginning or end of tweet\n",
    "reg_middle=re.compile(r'\\s#([a-z]{8,})\\s')\n",
    "reg_final=re.compile(r'\\s#([a-z]{8,})$')\n",
    "\n",
    "\n",
    "topics=[]\n",
    "for tweet in corpus:\n",
    "    # capture the tweets at the begining, middle and end of tweet\n",
    "    topics=topics+re.findall(reg_init,tweet)+re.findall(reg_middle,tweet)+re.findall(reg_final,tweet)\n",
    "\n",
    "print(len(topics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenise the hashtags. For this, instead of using a nltk tokenizer I will implement a reversed version of the MaxMatch algorithm, where matching begins at the end of the hashtag and progresses backwards (using NLTK list of words that for matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import sys\n",
    "words = nltk.corpus.words.words() # words is a Python list\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "tokenised_hashtags=[] # list of list with tokenised hashtags\n",
    "\n",
    "## code below based on the lemmatizer WSTA_N1B_preprocessing.ipynb modified to include adjectives and adverbs\n",
    "def lemmatize_word(word):\n",
    "    # try to lemmatize the word as verb, if not try with noun, adjective or adverb\n",
    "    lemma = lemmatizer.lemmatize(word,wn.VERB)\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,wn.NOUN)\n",
    "        if lemma == word:\n",
    "            lemma=lemmatizer.lemmatize(word,wn.ADJ)\n",
    "            if lemma == word:\n",
    "                lemma=lemmatizer.lemmatize(word,wn.ADV)\n",
    "    \n",
    "    return lemma\n",
    "## End of referenced code\n",
    "\n",
    "## Code below based in maxmatch algorithm on book Speech and Language Processing, pg.15\n",
    "def maxmatch(topic_word,tokenlist):\n",
    "\n",
    "    # if the word is empty we return the same string as the algorithm finished\n",
    "    if not topic_word:\n",
    "        return tokenlist\n",
    "    \n",
    "    # Get the first word and reminder\n",
    "    current_word=topic_word[0:len(topic_word)]\n",
    "    remainder=topic_word[0:0]\n",
    "    \n",
    "    for i in range(len(topic_word)):\n",
    "        current_word=topic_word[i:len(topic_word)] # Reading backwards and get reminder\n",
    "        remainder=topic_word[0:i]\n",
    "        \n",
    "        lemma=lemmatize_word(current_word) # get the lemma to search in word dictionary\n",
    "        \n",
    "        if lemma in words:\n",
    "            tokenlist.append(current_word)\n",
    "            return maxmatch(remainder,tokenlist) # recursive maxmatch algorithm with remainder\n",
    "            \n",
    "            \n",
    "    # if we can´t find a match with wordnet we will add the first letter and continue\n",
    "    current_word=topic_word[len(topic_word)-1]\n",
    "    remainder=topic_word[:len(topic_word)-1]\n",
    "    tokenlist.append(current_word)\n",
    "    \n",
    "    return maxmatch(remainder,tokenlist)\n",
    " ## End of algortithm reference    \n",
    "    \n",
    "for topic in topics: # go through every topic to create the list of lists\n",
    "    token_list=[]\n",
    "    token_list=maxmatch(topic,token_list) \n",
    "    tokenised_hashtags.append(token_list)\n",
    "        \n",
    "        \n",
    "tokenised_hashtags[-20:] # print the last 20"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the forward version of the MaxMatch algorithm and print out all the hashtags which give different results for the two versions of MaxMatch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import sys\n",
    "words = nltk.corpus.words.words() # words is a Python list\n",
    "\n",
    "tokenised_hashtags_forward=[] # list of list with tokenised hashtags\n",
    "\n",
    "## Code below based in maxmatch algorithm on book Speech and Language Processing, pg.15\n",
    "def maxmatch_forward(topic_word,tokenlist):\n",
    "\n",
    "    # if the word is empty we return the same string as the algorithm finished\n",
    "    if not topic_word:\n",
    "        return tokenlist\n",
    "    \n",
    "    # Get the first word and reminder\n",
    "    current_word=topic_word[0:len(topic_word)]\n",
    "    remainder=topic_word[0:0]\n",
    "    \n",
    "    for i in range(len(topic_word)):\n",
    "        current_word=topic_word[0:len(topic_word)-i] # Reading backwards and get reminder\n",
    "        remainder=topic_word[len(topic_word)-i:len(topic_word)]\n",
    "        lemma=lemmatize_word(current_word) # get the lemma to search in word dictionary\n",
    "        \n",
    "        if lemma in words:\n",
    "            tokenlist.append(current_word)\n",
    "            return maxmatch_forward(remainder,tokenlist) # recursive maxmatch algorithm with remainder\n",
    "            \n",
    "            \n",
    "    # if we can´t find a match with wordnet we will add the first letter and continue\n",
    "    current_word=topic_word[i]\n",
    "    remainder=topic_word[i:len(topic_word)-1]\n",
    "    tokenlist.append(current_word)\n",
    "    \n",
    "    return maxmatch_forward(remainder,tokenlist)\n",
    " ## End of algortithm reference    \n",
    "  \n",
    "for topic in topics: # go through every topic to create the list of lists\n",
    "    token_list_forward=[]\n",
    "    token_list_forward=maxmatch_forward(topic,token_list_forward) \n",
    "    tokenised_hashtags_forward.append(token_list_forward)\n",
    "    \n",
    "        \n",
    "tokenised_hashtags_forward[-20:] # print the las 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would use a mix of the two algorithms finding the best lemma for the use case I need. E.g. Start with the forward maxmatch and look for the lemma, if it satisfies, for example, it's a verb then continue with the forward, if not, try backwards to see if the new word can have a better fit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The twitter_sample corpus has two subcorpora corresponding to positive and negative tweets. Let's iterate through these two corpora and build training, development, and test sets for use with Scikit-learn. I will exclude stopwords (from the built-in NLTK list) and tokens with non-alphabetic characters (emoji). I will also use 80% of the tweets for training, 10% for development, and 10% for testing in a <i>stratified</i> way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import random\n",
    "\n",
    "positive_tweets = nltk.corpus.twitter_samples.tokenized(\"positive_tweets.json\") \n",
    "negative_tweets = nltk.corpus.twitter_samples.tokenized(\"negative_tweets.json\")\n",
    "\n",
    "    \n",
    "# remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "reg=re.compile('[^a-zA-Z]')\n",
    "filtered_positive_tweets=[]\n",
    "filtered_negative_tweets=[]\n",
    "\n",
    "# preprocess positive and negative tweets\n",
    "# remove stop words and words that are not matching lowercase letters\n",
    "for tweet in positive_tweets:        \n",
    "    filtered_positive_tweets.append([word for word in tweet if not word in stop_words and not re.match(reg,word)])\n",
    "\n",
    "for tweet in negative_tweets:\n",
    "    filtered_negative_tweets.append([word for word in tweet if not word in stop_words and not re.match(reg,word)])\n",
    "\n",
    "# randomize the tweets to split the train, dev and test sets \n",
    "# first, positives tweets\n",
    "random.shuffle(filtered_positive_tweets)\n",
    "\n",
    "# rules to assign train=80%, dev=10% and test=10%\n",
    "data_offset_train=int(round(len(filtered_positive_tweets)*0.80))\n",
    "data_offset_dev=int(round(data_offset_train+(len(filtered_positive_tweets)*0.10)))\n",
    "\n",
    "positive_tweets_train_set=filtered_positive_tweets[0:data_offset_train]\n",
    "positive_tweets_dev_set=filtered_positive_tweets[data_offset_train:data_offset_dev]\n",
    "positive_tweets_test_set=filtered_positive_tweets[data_offset_dev:len(filtered_positive_tweets)]\n",
    "\n",
    "\n",
    "# second, negatives tweets\n",
    "random.shuffle(filtered_negative_tweets)\n",
    "\n",
    "# rules to assign train=80%, dev=10% and test=10%\n",
    "data_offset_train=int(round(len(filtered_negative_tweets)*0.80))\n",
    "data_offset_dev=int(round(data_offset_train+(len(filtered_negative_tweets)*0.10)))\n",
    "\n",
    "negative_tweets_train_set=filtered_negative_tweets[0:data_offset_train]\n",
    "negative_tweets_dev_set=filtered_negative_tweets[data_offset_train:data_offset_dev]\n",
    "negative_tweets_test_set=filtered_negative_tweets[data_offset_dev:len(filtered_negative_tweets)]\n",
    "\n",
    "\n",
    "\n",
    "# create the dataset for traing, dev and test\n",
    "tweets_train_set=positive_tweets_train_set+negative_tweets_train_set\n",
    "random.shuffle(tweets_train_set)\n",
    "tweets_dev_set=positive_tweets_dev_set+negative_tweets_dev_set\n",
    "random.shuffle(tweets_dev_set)\n",
    "tweets_test_set=positive_tweets_test_set+negative_tweets_test_set\n",
    "random.shuffle(tweets_dev_set)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's build some classifiers. Here, we'll be comparing Naive Bayes and Logistic Regression. For each, I will first find a good value for their main regularisation (hyper)parameters using the development set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Process</h3>\n",
    "<b>\n",
    "1. Prepare the data and get the Document-Term Matrix for training, dev and test datasets\n",
    "2. Train models (Naive Bayes and Logistic Regression) and compare \n",
    "3. Test different parameters to understand how accuracy change\n",
    "\n",
    "</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vectorizer=DictVectorizer()\n",
    "\n",
    "# feature extraction with bag of words\n",
    "def get_BOW_lowered(tweet):\n",
    "    BOW = {}\n",
    "    for word in tweet:\n",
    "        word = word.lower()\n",
    "        BOW[word] = BOW.get(word,0) + 1\n",
    "    \n",
    "    return BOW\n",
    "\n",
    "# prepare the data and get the Document-Term Matrix for training, dev and test datasets\n",
    "def prepare_training_data(feature_extractor):\n",
    "    feature_matrix = []\n",
    "    classifications = []\n",
    "    for tweet in tweets_train_set: # go through every document in the training set\n",
    "        feature_dict = feature_extractor(tweet) #extract features from document\n",
    "        feature_matrix.append(feature_dict) # add features from document to a matrix\n",
    "        if tweet in filtered_negative_tweets: # builds a list with labels\n",
    "            classifications.append(\"negative\") \n",
    "        else:\n",
    "            classifications.append(\"positive\")\n",
    "            \n",
    "    # Create a document-term matrix learning the vocabulary\n",
    "    training_dtm = vectorizer.fit_transform(feature_matrix)\n",
    "    \n",
    "    return training_dtm,classifications\n",
    "\n",
    "def prepare_dev_data(feature_extractor):\n",
    "    feature_matrix = []\n",
    "    classifications = []\n",
    "    for tweet in tweets_dev_set: # go through every document in the dev set \n",
    "        feature_dict = feature_extractor(tweet) #extract features from document\n",
    "        feature_matrix.append(feature_dict) # add features from document to a matrix\n",
    "\n",
    "        if tweet in filtered_negative_tweets: # build a list with labels\n",
    "            classifications.append(\"negative\") \n",
    "        else:\n",
    "            classifications.append(\"positive\")\n",
    "  \n",
    "    # Create a document-term matrix\n",
    "    dev_dtm = vectorizer.transform(feature_matrix) \n",
    "    \n",
    "    return dev_dtm,classifications\n",
    "\n",
    "\n",
    "\n",
    "def prepare_test_data(feature_extractor):\n",
    "    feature_matrix = []\n",
    "    classifications = []\n",
    "    for tweet in tweets_test_set: # go through every document\n",
    "        feature_dict = feature_extractor(tweet) #extract features from document\n",
    "        feature_matrix.append(feature_dict) # add features from document to a matrix\n",
    "        \n",
    "        if tweet in filtered_negative_tweets: # build a list with labels\n",
    "            classifications.append(\"negative\") \n",
    "        else:\n",
    "            classifications.append(\"positive\")    \n",
    "\n",
    "    # create a document-term matrix\n",
    "    test_dtm = vectorizer.transform(feature_matrix) \n",
    "    \n",
    "    return test_dtm,classifications\n",
    "\n",
    "\n",
    "def check_results(predictions, classifications):\n",
    "    print(\"Accuracy:\")\n",
    "    print(accuracy_score(classifications,predictions))\n",
    "    print(classification_report(classifications,predictions))\n",
    "    print(confusion_matrix(classifications, predictions))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Train models (Naive Bayes and Logistic Regression) and compare</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def prepare_training_data():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# fit a logistic regression model to the data \n",
    "model = LogisticRegression()\n",
    "\n",
    "# get the train data\n",
    "X_train_dtm, y_train=prepare_training_data(get_BOW_lowered)\n",
    "model.fit(X_train_dtm, y_train)\n",
    "y_predicted_class = model.predict(X_train_dtm)\n",
    "\n",
    "# view results for Logistic Regression not tuned - training dataset\n",
    "print(\"Logistic Regression not tuned - Training\")\n",
    "check_results(y_predicted_class,y_train)\n",
    "\n",
    "# test the model with dev dataset\n",
    "X_dev_dtm,y_dev=prepare_dev_data(get_BOW_lowered)\n",
    "y_predicted_class = model.predict(X_dev_dtm)\n",
    "\n",
    "print(\"-----------------------------------------\")\n",
    "\n",
    "# view results for Logistic Regression not tuned - dev dataset\n",
    "print(\"Logistic Regression not tuned - Dev\")\n",
    "check_results(y_predicted_class,y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# train fit a Naive Bayes model \n",
    "model = MultinomialNB()\n",
    "X_train_dtm, y_train=prepare_training_data(get_BOW_lowered)\n",
    "\n",
    "# train the model using X_train_dtm \n",
    "model.fit(X_train_dtm, y_train)\n",
    "y_predicted_class = model.predict(X_train_dtm)\n",
    "print(\"Naive Bayes not tuned  - Training\")\n",
    "check_results(y_predicted_class,y_train)\n",
    "\n",
    "# test the model with dev dataset\n",
    "X_dev_dtm,y_dev=prepare_dev_data(get_BOW_lowered)\n",
    "y_predicted_class = model.predict(X_dev_dtm)\n",
    "\n",
    "# view results for Naive Bayes not tuned - dev dataset\n",
    "print(\"Naive Bayes not tuned  - Dev\")\n",
    "check_results(y_predicted_class,y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Conclusions</h5>\n",
    "<b>The model is performing relatively well with training data but not so good with the development dataset. With the actual parameters the model is overfitting. I will try to use regularization to avoid overfitting tuning the complexity of the model. I'll focus first trying with different values for parameter $\\lambda$ in L2 Regularization for LogisticRegression and $\\alpha$ to smooth Naive Bayes and remove $0$ probabilites.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Test different parameters to understand how accuracy change</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Model hyper-parameter $C = \\frac{1}{\\lambda}$ controls how the complexity is handled. I will sample values of $C$ low and close to $0$ so the model can increase in complexity assigning big values to the weights for each parameter in the model. I will also decrease $\\lambda$ using higher values of $C$ to simplify the model and compare the accuracy for training vs dev dataset.</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "param_C=list(np.power(10.0, np.arange(-15, 15))) # a vector with c values\n",
    "df_params=pd.DataFrame(columns=['C','Training Accuracy','Dev Accuracy']) # create a dataframe to show results and comparison\n",
    "row=len(df_params)\n",
    "for c in param_C:\n",
    "    model=LogisticRegression(C=c) # A small improve is seen when we eliminate intercept (bias)\n",
    "    # train the model using X_train_dtm and make predictions with training data\n",
    "    model.fit(X_train_dtm, y_train)\n",
    "    y_train_predicted_class = model.predict(X_train_dtm)\n",
    "    \n",
    "    # make predictions with dev data\n",
    "    y_dev_predicted_class = model.predict(X_dev_dtm)\n",
    "    \n",
    "    # save results in dataframe\n",
    "    df_params.loc[row+1]=[c,accuracy_score(y_train,y_train_predicted_class),accuracy_score(y_dev,y_dev_predicted_class)]\n",
    "    row=row+1\n",
    "    \n",
    "best_c=(df_params[df_params['Dev Accuracy'] == max(df_params['Dev Accuracy'])]).iat[0,0]\n",
    "print(df_params)\n",
    "print(\"Best parameter C:\" +str(best_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The maximum accuracy for our dev and training dataset is shown on the table (changing the fit_intercept may improve a bit).\n",
    "I tried other parameters but the performance was not improved.<br/> I will use different approaches to tune the parameters after perform the same operation with Naive Bayes.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector with alpha values\n",
    "param_alpha=list(np.arange(0.1,5,0.2))\n",
    "df_params=pd.DataFrame(columns=['Alpha','Training Accuracy','Dev Accuracy'])\n",
    "row=len(df_params)\n",
    "for alpha in param_alpha:\n",
    "    model=MultinomialNB(alpha=alpha,fit_prior=True) # fit_prior = False decrease the accuracy for dev set\n",
    "    # train the model using X_train_dtm \n",
    "    model.fit(X_train_dtm, y_train)\n",
    "    # make predictions with training data\n",
    "    y_train_predicted_class = model.predict(X_train_dtm)\n",
    "    # make predictions with dev data\n",
    "    y_dev_predicted_class = model.predict(X_dev_dtm)\n",
    "    df_params.loc[row+1]=[alpha,accuracy_score(y_train,y_train_predicted_class),accuracy_score(y_dev,y_dev_predicted_class)]\n",
    "    row=row+1\n",
    "    \n",
    "best_alpha=(df_params[df_params['Dev Accuracy'] == max(df_params['Dev Accuracy'])]).iat[0,0]\n",
    "print(df_params)\n",
    "print(\"Best parameter alpha:\" +str(best_alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The maximum accuracy for our dev and training dataset is shown on the table.\n",
    "(I tried other parameters but the performance was not improved).<br/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>5. Test different approaches for tunning </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search to find the best parameters\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_C=list(np.power(10.0, np.arange(-15, 15))) # a list of Cs params to include in the parameter grid\n",
    "fit_intercept_params=[True,False]\n",
    "\n",
    "# create and fit a logistic regression model, testing each C param with different fit intercepts\n",
    "model = LogisticRegression()\n",
    "grid = GridSearchCV(estimator=model, param_grid=dict(C=param_C,fit_intercept=fit_intercept_params) )\n",
    "grid.fit(X_train_dtm, y_train)\n",
    "y_dev_predicted_class=grid.predict(X_dev_dtm)\n",
    "\n",
    "# summarize the results of the grid search\n",
    "print(\"Best accuracy: \"+str(grid.best_score_))\n",
    "print(\"Best estimator C: \"+str(grid.best_estimator_.C))\n",
    "print(\"Best parameters: \"+str(grid.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search for Algorithm Tuning\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# prepare a range of alpha values to test\n",
    "alphas = list(np.arange(0.1,5,0.2))\n",
    "\n",
    "# fit the model with alpha parameter grid\n",
    "model = MultinomialNB()\n",
    "grid = GridSearchCV(estimator=model, param_grid=dict(alpha=alphas))\n",
    "grid.fit(X_train_dtm, y_train)\n",
    "y_predicted_class=grid.predict(X_dev_dtm)\n",
    "print(\"Best parameters: \"+str(grid.best_params_))\n",
    "# view the results of the grid search\n",
    "print(\"Best accuracy: \"+str(grid.best_score_))\n",
    "print(\"Best estimator Alpha: \"+str(grid.best_estimator_.alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomized search to look for parameter combination for the Logistic Regression \n",
    "import scipy\n",
    "from scipy.stats import uniform as sp_rand\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# sample values of C from a distribution and including other parameters in the grid\n",
    "param_grid = {'C': scipy.stats.expon(scale=100),'class_weight':['balanced', None], 'max_iter':[50,100,200],'fit_intercept':[True,False]}\n",
    "\n",
    "# fit the model using a randomized parameter grid\n",
    "model = LogisticRegression()\n",
    "rsearch = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=300)\n",
    "rsearch.fit(X_train_dtm, y_train)\n",
    "y_predicted_class=rsearch.predict(X_dev_dtm)\n",
    "\n",
    "# view the results\n",
    "print(\"Best accuracy: \"+str(rsearch.best_score_))\n",
    "print(\"Best estimator C: \"+str(rsearch.best_estimator_.C))\n",
    "print(\"Best parameters: \"+str(rsearch.best_params_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomized search to look for parameter combination for the Multinomial NB\n",
    "from scipy.stats import uniform as sp_rand\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# sample the parameter alpha\n",
    "param_grid = {'alpha': sp_rand()}\n",
    "\n",
    "# fit the model using a randomized parameter grid\n",
    "model = MultinomialNB()\n",
    "rsearch = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=300)\n",
    "rsearch.fit(X_train_dtm, y_train)\n",
    "y_predicted_class=rsearch.predict(X_dev_dtm)\n",
    "\n",
    "# view the results\n",
    "print(\"Best accuracy: \"+str(rsearch.best_score_))\n",
    "print(\"Best estimator Alpha: \"+str(rsearch.best_estimator_.alpha))\n",
    "print(\"Best parameters: \"+str(rsearch.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using LogisticRegressionCV for customized grid search in model LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "# create the model with different parameters. Fit and predict.\n",
    "model = LogisticRegressionCV(\n",
    "    Cs=list(np.arange(0.1, 10, 0.5))\n",
    "    ,penalty='l2'\n",
    "    ,random_state=777\n",
    "    ,max_iter=10000\n",
    "    ,fit_intercept=True\n",
    "    ,solver='liblinear'\n",
    "    ,tol=0.0001\n",
    ")\n",
    "model.fit(X_train_dtm, y_train)\n",
    "y_predicted_class=model.predict(X_dev_dtm)\n",
    "\n",
    "# view the results\n",
    "check_results(y_predicted_class,y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compare with the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "\n",
    "# get the test data to run the model\n",
    "X_train_dtm, y_train=prepare_training_data(get_BOW_lowered)\n",
    "X_test_dtm,y_test=prepare_test_data(get_BOW_lowered)\n",
    "\n",
    "# fit a logistic regression model to the data using the best parameters found\n",
    "model_regression = LogisticRegression(C=best_c, fit_intercept= False)\n",
    "\n",
    "# train the model using X_train_dtm \n",
    "model_regression.fit(X_train_dtm, y_train)\n",
    "\n",
    "# make predictions with the testing data\n",
    "y_regression_predicted_class = model_regression.predict(X_test_dtm)\n",
    "\n",
    "\n",
    "\n",
    "# fit a Naive Bayes model to the data\n",
    "model_NB = MultinomialNB(alpha=best_alpha)\n",
    "\n",
    "# train the model using X_train_dtm \n",
    "model_NB.fit(X_train_dtm, y_train)\n",
    "y_nb_predicted_class = model_NB.predict(X_test_dtm)\n",
    "\n",
    "# summary results\n",
    "# create a dataframe to put the results\n",
    "df_model_results=pd.DataFrame(columns=['Model','Accuracy','Macroaveraged F-Score'])\n",
    "df_model_results.loc[len(df_model_results)]=['LinearRegression',accuracy_score(y_test,y_regression_predicted_class),f1_score(y_test, y_regression_predicted_class, average=\"macro\")]\n",
    "df_model_results.loc[len(df_model_results)]=['Naive Bayes',accuracy_score(y_test,y_nb_predicted_class),f1_score(y_test, y_nb_predicted_class, average=\"macro\")]\n",
    "\n",
    "print(\"********Summary************\")\n",
    "print(df_model_results)\n",
    "print(\"***************************\")\n",
    "# results for the tunned logistic regression\n",
    "print(\"Logistic Regression - Test\")\n",
    "check_results(y_regression_predicted_class,y_test)\n",
    "\n",
    "# view the results for the tunned NB classifier\n",
    "print(\"Naive Bayes - Test\")\n",
    "check_results(y_nb_predicted_class,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
