{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll be will be comparing our methods against a popular dataset of word similarities called Similarity-353. The dataset can be downloaded <a href=\"http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/wordsim353.zip\">here</a>. The file we will be using is called *combined.tab*. Except for the header (which should be stripped out), the file is tab formated with the first two columns corresponding to two words, and the third column representing a human-annotated similarity between the two words.\n",
    "\n",
    "I will filter this dataset to generate a smaller test set where I will evaluate word similarity methods.\n",
    "\n",
    "The first filtering is based on document frequencies in the Brown corpus, in order to remove rare words. I will be treating the <i>paragraphs</i> of the Brown corpus as our \"documents\", the words will be lower-cased and lemmatized before they are added to the set. Then, using the information in this corpus, I will calculate document frequencies and remove from test set any word pairs where at least one of the two words has a document frequency of less than 10 in this corpus. \n",
    "\n",
    "The second filtering is based on words with highly ambiguous senses and involves using the NLTK interface to WordNet. Here, I remove any words which do not have a *single primary sense*. We define single primary sense here as either having only one sense (i.e. only one synset), or where the count (as provided by the WordNet `count()` method for the lemmas associated with a synset) of the most common sense is at least five and at least five times larger than the next most common sense. Also, I remove any words where the primary sense is not a noun (this information is also in the synset). Store the synset corresponding to this primary sense in a dictionary for use in the next section. I will remove any word pairs from the test set where at least one of the words does not contain a single primary sense or if the single primary sense is not a noun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{('aluminum', 'metal'): 7.83,\n",
       " ('baby', 'mother'): 7.85,\n",
       " ('brother', 'monk'): 6.27,\n",
       " ('canyon', 'landscape'): 7.53,\n",
       " ('car', 'automobile'): 8.94,\n",
       " ('century', 'year'): 7.59,\n",
       " ('coast', 'forest'): 3.15,\n",
       " ('coast', 'hill'): 4.38,\n",
       " ('coast', 'shore'): 9.1,\n",
       " ('computer', 'laboratory'): 6.78,\n",
       " ('doctor', 'personnel'): 5.0,\n",
       " ('drink', 'car'): 3.04,\n",
       " ('drink', 'ear'): 1.31,\n",
       " ('drink', 'mother'): 2.65,\n",
       " ('equipment', 'maker'): 5.91,\n",
       " ('hotel', 'reservation'): 8.03,\n",
       " ('journey', 'car'): 5.85,\n",
       " ('journey', 'voyage'): 9.29,\n",
       " ('luxury', 'car'): 6.47,\n",
       " ('money', 'cash'): 9.08,\n",
       " ('monk', 'slave'): 0.92,\n",
       " ('phone', 'equipment'): 7.13,\n",
       " ('planet', 'people'): 5.75,\n",
       " ('president', 'medal'): 3.0,\n",
       " ('professor', 'doctor'): 6.62,\n",
       " ('psychology', 'doctor'): 6.42,\n",
       " ('psychology', 'fear'): 6.85,\n",
       " ('psychology', 'health'): 7.23,\n",
       " ('psychology', 'mind'): 7.69,\n",
       " ('psychology', 'science'): 6.71,\n",
       " ('school', 'center'): 3.44,\n",
       " ('soap', 'opera'): 7.94,\n",
       " ('stock', 'egg'): 1.81,\n",
       " ('stock', 'phone'): 1.62,\n",
       " ('train', 'car'): 6.31,\n",
       " ('type', 'kind'): 8.97,\n",
       " ('weapon', 'secret'): 6.06,\n",
       " ('word', 'similarity'): 4.75}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load combined.tab into a dictionary\n",
    "import csv\n",
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk import FreqDist\n",
    "import numpy as np\n",
    "\n",
    "dataset={}\n",
    "dataset_filtered_1={}\n",
    "dataset_filtered_2={}\n",
    "document_sets=[]   \n",
    "\n",
    "file_name='combined.tab'\n",
    "\n",
    "\n",
    "\n",
    "def lemmatize_word(word):\n",
    "    lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "    # try to lemmatize the word as verb, if not try with noun, adjective or adverb\n",
    "    lemma = lemmatizer.lemmatize(word,wn.NOUN)\n",
    "    \n",
    "    return lemma\n",
    "\n",
    "\n",
    "def single_primary_sense(word):\n",
    "    synsets_word=wn.synsets(word)\n",
    "    count_lemmas_next_common_sense=0\n",
    "    count_lemmas_most_common_sense=0\n",
    "    \n",
    "    # remove any words which don´t have a single primary sense (either having only one sense like one synset )\n",
    "    if synsets_word: \n",
    "        if len(synsets_word)==1: # means the word only have one set (and it´s a noun synset)\n",
    "            return True\n",
    "        # remove the word if the most common sense is not at least\n",
    "        # five and at least five times larger thant the next most common sense\n",
    "        if (synsets_word[0].lemmas()):\n",
    "            for lemma in synsets_word[0].lemmas():\n",
    "                # taking the name of first synset lemma and compare it to every lemma in second synset. \n",
    "                # if matches then I use the lemma count as valid for the next common sense lemma count\n",
    "                if (lemma.name().split()[-1]==word):\n",
    "                    count_lemmas_most_common_sense+=lemma.count()\n",
    "\n",
    "        if len(synsets_word)>1: # does the word have a next sense?\n",
    "            for lemma in synsets_word[1].lemmas():\n",
    "                # taking the name of first synset lemma and compare it to every lemma in second synset. \n",
    "                # if matches then I use the lemma count as valid for the next common sense lemma count\n",
    "                if (lemma.name().split()[-1]==word):\n",
    "                    count_lemmas_next_common_sense+=lemma.count()\n",
    "\n",
    "\n",
    "        if count_lemmas_most_common_sense>=5 and count_lemmas_most_common_sense>=(5*count_lemmas_next_common_sense):\n",
    "            return True\n",
    "            \n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "def getDatasetDictionary():\n",
    "    dataset_dict={}\n",
    "    with open(file_name, 'r') as file:\n",
    "        combined_reader = csv.reader(file, delimiter='\\t')\n",
    "        next(combined_reader) # discards the header\n",
    "        for line in combined_reader:\n",
    "            dataset_dict[line[0],line[1]]=float(line[2])\n",
    "    \n",
    "    return dataset_dict\n",
    "\n",
    "\n",
    "def get_documents_set():\n",
    "    documents_set=[]\n",
    "    # got through Paragraphs > Sentences > words\n",
    "    for para in brown.paras():\n",
    "        document=set()\n",
    "        for sentence in para:\n",
    "            for word in sentence:\n",
    "                word=lemmatize_word(word.lower())\n",
    "                document.add(word)\n",
    "        \n",
    "        documents_set.append(document)\n",
    "    return documents_set\n",
    "\n",
    "def has_primary_noun_synset(word):\n",
    "    synsets=wn.synsets(word)\n",
    "    synsets_noun=wn.synsets(word,wn.NOUN)\n",
    "    if len(synsets)>0 and len(synsets_noun)>0:\n",
    "        if synsets[0]==synsets_noun[0]:\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "# document frequency for a word x is a number of documents in which x appears\n",
    "def get_term_document_frequency(documents_set):\n",
    "    term_document_frequency={}\n",
    "    for document in documents_set:\n",
    "        for word in document:\n",
    "            term_document_frequency[word]=term_document_frequency.get(word,0) + 1\n",
    "    return term_document_frequency\n",
    "\n",
    "def get_dataset_filtered_1(ds,term_document_frequency):\n",
    "    \n",
    "    dataset_filtered={}\n",
    "                \n",
    "    for key in ds: \n",
    "        word_1=key[0].lower() # No need to lemmatize or preprocessing to do.\n",
    "        word_2=key[1].lower()\n",
    "        \n",
    "        frequency_word1=0\n",
    "        frequency_word2=0\n",
    "        frequency_word1=term_document_frequency.get(word_1,0)\n",
    "        frequency_word2=term_document_frequency.get(word_2,0)\n",
    "        if frequency_word1 >= 10 and frequency_word2 >= 10: \n",
    "            dataset_filtered[key]=ds[key]\n",
    "\n",
    "    return dataset_filtered\n",
    "\n",
    "def get_dataset_filtered_2(ds):\n",
    "    dataset_filtered={}\n",
    "    for key in ds:\n",
    "        word_1=key[0]\n",
    "        word_2=key[1]\n",
    "        if single_primary_sense(word_1) and single_primary_sense(word_2): \n",
    "            # to remove the words which single primary sinsets are not nouns\n",
    "            if (has_primary_noun_synset(word_1) and has_primary_noun_synset(word_2)):\n",
    "                dataset_filtered[key]=ds[key]\n",
    "    \n",
    "    return dataset_filtered\n",
    "\n",
    "# get the dataset dictionary\n",
    "dataset=getDatasetDictionary()\n",
    "\n",
    "# get documents simplified\n",
    "document_sets=get_documents_set()\n",
    "term_document=get_term_document_frequency(document_sets)\n",
    "dataset_filtered_1=get_dataset_filtered_1(dataset,term_document)\n",
    "dataset_filtered_2=get_dataset_filtered_2(dataset_filtered_1)\n",
    "\n",
    "# print out all the pairs in the filtered test set. 10 lt Total lt 50 \n",
    "print(len(dataset_filtered_2))\n",
    "dataset_filtered_2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create similarity scores for pairs of words in the test set. The first of these is the Wu-Palmer scores derived from the hypernym relationships in WordNet calculated using the primary sense for each word derived above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('aluminum', 'metal'): 0.9333333333333333,\n",
       " ('baby', 'mother'): 0.5,\n",
       " ('brother', 'monk'): 0.5714285714285714,\n",
       " ('canyon', 'landscape'): 0.3333333333333333,\n",
       " ('car', 'automobile'): 1.0,\n",
       " ('century', 'year'): 0.8333333333333334,\n",
       " ('coast', 'forest'): 0.16666666666666666,\n",
       " ('coast', 'hill'): 0.6666666666666666,\n",
       " ('coast', 'shore'): 0.9090909090909091,\n",
       " ('computer', 'laboratory'): 0.35294117647058826,\n",
       " ('doctor', 'personnel'): 0.13333333333333333,\n",
       " ('drink', 'car'): 0.1111111111111111,\n",
       " ('drink', 'ear'): 0.13333333333333333,\n",
       " ('drink', 'mother'): 0.11764705882352941,\n",
       " ('equipment', 'maker'): 0.5,\n",
       " ('hotel', 'reservation'): 0.375,\n",
       " ('journey', 'car'): 0.09523809523809523,\n",
       " ('journey', 'voyage'): 0.8571428571428571,\n",
       " ('luxury', 'car'): 0.1111111111111111,\n",
       " ('money', 'cash'): 0.8,\n",
       " ('monk', 'slave'): 0.6666666666666666,\n",
       " ('phone', 'equipment'): 0.875,\n",
       " ('planet', 'people'): 0.18181818181818182,\n",
       " ('president', 'medal'): 0.11764705882352941,\n",
       " ('professor', 'doctor'): 0.5,\n",
       " ('psychology', 'doctor'): 0.1111111111111111,\n",
       " ('psychology', 'fear'): 0.25,\n",
       " ('psychology', 'health'): 0.21052631578947367,\n",
       " ('psychology', 'mind'): 0.5714285714285714,\n",
       " ('psychology', 'science'): 0.9411764705882353,\n",
       " ('school', 'center'): 0.13333333333333333,\n",
       " ('soap', 'opera'): 0.2222222222222222,\n",
       " ('stock', 'egg'): 0.11764705882352941,\n",
       " ('stock', 'phone'): 0.125,\n",
       " ('train', 'car'): 0.6666666666666666,\n",
       " ('type', 'kind'): 0.9473684210526315,\n",
       " ('weapon', 'secret'): 0.13333333333333333,\n",
       " ('word', 'similarity'): 0.3333333333333333}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create similarity scores with Wu-Palmer drived from hypernym relationships in WordNet\n",
    "dataset_wu_palmer=dataset_filtered_2.copy()\n",
    "# populate the wu_palmer dictionary with the words derived from the filtered dataset\n",
    "for key in dataset_wu_palmer:\n",
    "    word_1=key[0]\n",
    "    word_2=key[1]\n",
    "    \n",
    "    synsets_word_1=wn.synsets(word_1)\n",
    "    synsets_word_2=wn.synsets(word_2)\n",
    "    \n",
    "    \n",
    "    # similarity calulated using most common sense for each word\n",
    "    similarity=synsets_word_1[0].wup_similarity(synsets_word_2[0])\n",
    "    dataset_wu_palmer[key]=similarity\n",
    "    \n",
    "        \n",
    "\n",
    "# print the python dictionary of word pair / similarity\n",
    "dataset_wu_palmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's calculate Positive PMI (PPMI) for your word pairs using statistics derived from the Brown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PMI between a target word $w$ and a context $c$ <br/><br/>\n",
    "$pmi(w,c) = log_2 \\frac{P(w,c)}{P(w)P(c)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('aluminum', 'metal'): 5.195660721157932,\n",
       " ('baby', 'mother'): 3.1068514542000756,\n",
       " ('brother', 'monk'): 2.8992677183777067,\n",
       " ('canyon', 'landscape'): 0.0,\n",
       " ('car', 'automobile'): 3.284928059255019,\n",
       " ('century', 'year'): 0.85521193298008,\n",
       " ('coast', 'forest'): 3.0505076829814297,\n",
       " ('coast', 'hill'): 1.2130606957673897,\n",
       " ('coast', 'shore'): 4.630747773460183,\n",
       " ('computer', 'laboratory'): 0.0,\n",
       " ('doctor', 'personnel'): 2.2186218696012423,\n",
       " ('drink', 'car'): 0.8109968709226062,\n",
       " ('drink', 'ear'): 0.0,\n",
       " ('drink', 'mother'): 0.7957301142692957,\n",
       " ('equipment', 'maker'): 4.283313403192924,\n",
       " ('hotel', 'reservation'): 2.891047211572738,\n",
       " ('journey', 'car'): 0.0,\n",
       " ('journey', 'voyage'): 0.0,\n",
       " ('luxury', 'car'): 2.272328022475385,\n",
       " ('money', 'cash'): 2.5276424807092166,\n",
       " ('monk', 'slave'): 0.0,\n",
       " ('phone', 'equipment'): 0.0,\n",
       " ('planet', 'people'): 0.4092477799069862,\n",
       " ('president', 'medal'): 0.0,\n",
       " ('professor', 'doctor'): 0.0,\n",
       " ('psychology', 'doctor'): 3.5625762708186035,\n",
       " ('psychology', 'fear'): 0.0,\n",
       " ('psychology', 'health'): 0.0,\n",
       " ('psychology', 'mind'): 2.7796743924855387,\n",
       " ('psychology', 'science'): 5.078497127110109,\n",
       " ('school', 'center'): 0.744045575429721,\n",
       " ('soap', 'opera'): 4.221195813265069,\n",
       " ('stock', 'egg'): 1.8174736272140593,\n",
       " ('stock', 'phone'): 0.0,\n",
       " ('train', 'car'): 0.9714615431158522,\n",
       " ('type', 'kind'): 0.6500752376975433,\n",
       " ('weapon', 'secret'): 2.634516839954891,\n",
       " ('word', 'similarity'): 0.0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def get_ppmi_score(x, y):\n",
    "    \n",
    "    count_x=0.0\n",
    "    count_y=0.0\n",
    "    total_count=0.0\n",
    "    count_x_y=0.0\n",
    "    p_x_y=0.0\n",
    "    p_x=0.0\n",
    "    p_y=0.0\n",
    "    pmi_x_y=0.0\n",
    "    \n",
    "    for document in document_sets:\n",
    "        x_in_document=False\n",
    "        y_in_document=False\n",
    "        \n",
    "        for word in document:\n",
    "\n",
    "            if word==x:\n",
    "                count_x+=1\n",
    "            \n",
    "                x_in_document=True\n",
    "            elif word==y:\n",
    "                count_y+=1\n",
    "                total_count+=1\n",
    "                y_in_document=True\n",
    "            \n",
    "        if x_in_document and y_in_document:\n",
    "            count_x_y+=1     \n",
    "        \n",
    "    p_x_y=count_x_y/len(document_sets)\n",
    "    p_x=count_x/len(document_sets)\n",
    "    p_y=count_y/len(document_sets) \n",
    "        \n",
    "    if (p_x*p_y*p_x_y>0):     \n",
    "        # if log is less than 0 means words are co-occurring less than we expect by chance\n",
    "        pmi_x_y=max(math.log(p_x_y/(p_x*p_y),2),0)  \n",
    "    \n",
    "    return pmi_x_y\n",
    "\n",
    "\n",
    "# get a copy of the dataset to fill word pairs with ppmi\n",
    "dataset_ppmi=dataset_filtered_2.copy()\n",
    "\n",
    "# calculate the ppmi for every pair in our dataset\n",
    "for word_pair in dataset_ppmi:\n",
    "    word_1=lemmatize_word(word_pair[0].lower()) # lemmatizing since we are going to compare with the documents\n",
    "    word_2=lemmatize_word(word_pair[1].lower())\n",
    "    \n",
    "    dataset_ppmi[word_pair]=get_ppmi_score(word_1, word_2)\n",
    "    \n",
    "\n",
    "dataset_ppmi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I will derive similarity scores using the LSA method, i.e. apply SVD and truncate to get a dense vector and then use cosine similarity between the two vectors for each word pair.I wil use truncatedSVD in Sci-kit learn to produce dense vectors of length 500, and then use cosine similarity to produce similarities for your word pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('aluminum', 'metal'): 0.23649550473014069,\n",
       " ('baby', 'mother'): 0.33243164151288696,\n",
       " ('brother', 'monk'): 0.074407863741779323,\n",
       " ('canyon', 'landscape'): 0.11213145179354456,\n",
       " ('car', 'automobile'): 0.34651907546962046,\n",
       " ('century', 'year'): 0.071588465579824456,\n",
       " ('coast', 'forest'): 0.10877911187910105,\n",
       " ('coast', 'hill'): 0.19527250376223726,\n",
       " ('coast', 'shore'): 0.39241131023582776,\n",
       " ('computer', 'laboratory'): 0.12970301058889433,\n",
       " ('doctor', 'personnel'): 0.047100581556278276,\n",
       " ('drink', 'car'): 0.10412288328340535,\n",
       " ('drink', 'ear'): 0.071306873593828479,\n",
       " ('drink', 'mother'): 0.069475474873495258,\n",
       " ('equipment', 'maker'): 0.27665719864367888,\n",
       " ('hotel', 'reservation'): 0.063667638924678194,\n",
       " ('journey', 'car'): -0.018161672088413905,\n",
       " ('journey', 'voyage'): 0.13919117050178054,\n",
       " ('luxury', 'car'): 0.10307725619342645,\n",
       " ('money', 'cash'): 0.1454837975098745,\n",
       " ('monk', 'slave'): -0.044018597810947524,\n",
       " ('phone', 'equipment'): 0.011789448079946419,\n",
       " ('planet', 'people'): 0.032932824244804593,\n",
       " ('president', 'medal'): -0.013863259045444387,\n",
       " ('professor', 'doctor'): 0.054363987680835169,\n",
       " ('psychology', 'doctor'): 0.13431371769471823,\n",
       " ('psychology', 'fear'): 0.1139213810086821,\n",
       " ('psychology', 'health'): 0.047882867674061136,\n",
       " ('psychology', 'mind'): 0.11882210787580183,\n",
       " ('psychology', 'science'): 0.27371464921025401,\n",
       " ('school', 'center'): 0.052013653881409216,\n",
       " ('soap', 'opera'): 0.012155027781481809,\n",
       " ('stock', 'egg'): 0.10848294216522802,\n",
       " ('stock', 'phone'): 0.034842487019946855,\n",
       " ('train', 'car'): 0.079308111611179344,\n",
       " ('type', 'kind'): 0.019266044152039141,\n",
       " ('weapon', 'secret'): 0.17482080275583112,\n",
       " ('word', 'similarity'): 0.0033989308147740482}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "\n",
    "# bag of words to use it with every document\n",
    "def get_BOW(text):\n",
    "    BOW = {}  \n",
    "    for word in text:\n",
    "        BOW[word.lower()] = BOW.get(word.lower(),0) + 1\n",
    "    \n",
    "    return BOW\n",
    "\n",
    "# list of dictionaries with BOW for every document set\n",
    "texts = []\n",
    "for document in document_sets:\n",
    "    texts.append(get_BOW(document))\n",
    "\n",
    "\n",
    "# get the term-document matrix\n",
    "vectorizer = DictVectorizer()\n",
    "brown_matrix = vectorizer.fit_transform(texts).T\n",
    "\n",
    "# applying SVD\n",
    "svd = TruncatedSVD(n_components=500)\n",
    "brown_matrix = svd.fit_transform(brown_matrix)\n",
    "\n",
    "# dataset with cosine similarities\n",
    "dataset_cosine_similarity=dataset_filtered_2.copy()\n",
    "\n",
    "for key in dataset_cosine_similarity:\n",
    "    vector_word_1=[]\n",
    "    vector_word_2=[]\n",
    "    \n",
    "    # scan the features to find the word we are looking for\n",
    "    for feature in vectorizer.get_feature_names():    \n",
    "        if key[0]==feature:\n",
    "            vector_word_1=brown_matrix[vectorizer.vocabulary_.get(key[0])] # retrieve the word vector from matrix\n",
    "        if key[1]==feature:\n",
    "            vector_word_2=brown_matrix[vectorizer.vocabulary_.get(key[1])] # retrieve the word vector from matrix\n",
    "        if len(vector_word_1)>0 and len(vector_word_2)>0:\n",
    "            break\n",
    "        \n",
    "    # get cosine similarity between vectors and add it to the dataset\n",
    "    distance=cosine(vector_word_1,vector_word_2)\n",
    "    dataset_cosine_similarity[key]=1-distance \n",
    "\n",
    "dataset_cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will derive a similarity score from word2vec vectors, using the Gensim interface. Check the Gensim word2vec tutorial for details on the API: https://radimrehurek.com/gensim/models/word2vec.html. My vectors will have the same number of dimensions as LSA (500), and run for 50 iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('aluminum', 'metal'): 0.61249731059222601,\n",
       " ('baby', 'mother'): 0.24806951434605501,\n",
       " ('brother', 'monk'): 0.18979339329331829,\n",
       " ('canyon', 'landscape'): 0.027469219789351452,\n",
       " ('car', 'automobile'): 0.24388802172656793,\n",
       " ('century', 'year'): 0.36964380627284538,\n",
       " ('coast', 'forest'): 0.21735449863030148,\n",
       " ('coast', 'hill'): 0.46980546329244566,\n",
       " ('coast', 'shore'): 0.43456961154981077,\n",
       " ('computer', 'laboratory'): 0.12874616994648233,\n",
       " ('doctor', 'personnel'): 0.14877272730870961,\n",
       " ('drink', 'car'): 0.17745198587180427,\n",
       " ('drink', 'ear'): 0.21360208008965711,\n",
       " ('drink', 'mother'): 0.18511475031732816,\n",
       " ('equipment', 'maker'): 0.23870496809580691,\n",
       " ('hotel', 'reservation'): 0.17949129613603371,\n",
       " ('journey', 'car'): 0.17750559599260754,\n",
       " ('journey', 'voyage'): 0.46100917017950932,\n",
       " ('luxury', 'car'): 0.10760435657073361,\n",
       " ('money', 'cash'): 0.30738483667156852,\n",
       " ('monk', 'slave'): 0.19510641745750062,\n",
       " ('phone', 'equipment'): -0.036527752741762606,\n",
       " ('planet', 'people'): 0.062888238420428011,\n",
       " ('president', 'medal'): 0.068606374805547293,\n",
       " ('professor', 'doctor'): 0.077138415146297434,\n",
       " ('psychology', 'doctor'): 0.073630520117220002,\n",
       " ('psychology', 'fear'): 0.088052219548433744,\n",
       " ('psychology', 'health'): 0.14425087707030942,\n",
       " ('psychology', 'mind'): 0.021070277483463656,\n",
       " ('psychology', 'science'): 0.3086320436078297,\n",
       " ('school', 'center'): 0.093467940462704943,\n",
       " ('soap', 'opera'): -0.016868794170238455,\n",
       " ('stock', 'egg'): 0.14799741062319741,\n",
       " ('stock', 'phone'): -0.081902581138642724,\n",
       " ('train', 'car'): 0.31076669916920252,\n",
       " ('type', 'kind'): 0.28515702096529122,\n",
       " ('weapon', 'secret'): 0.20338189747142682,\n",
       " ('word', 'similarity'): -0.054125536840008676}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "# copy the original dataset to populate with similarity\n",
    "dataset_word2vec_similarity=dataset_filtered_2.copy()\n",
    "\n",
    "# get all sentences to train the model\n",
    "brown_sents=[]\n",
    "for sent in brown.sents():\n",
    "    brown_sents.append([lemmatize_word(word.lower()) for word in sent]) # lemmatize every word\n",
    "    \n",
    "# train the model\n",
    "model = gensim.models.Word2Vec(brown_sents, size=500, iter=50, workers=4)\n",
    "\n",
    "# get the similarity using the model\n",
    "for key in dataset_word2vec_similarity:\n",
    "    dataset_word2vec_similarity[key]=model.wv.similarity(key[0], key[1])\n",
    "   \n",
    "# print similarities\n",
    "dataset_word2vec_similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Finally, let's should compare all the similarities  to the gold standard loaded and filtered in the first step. For this, you can use the Pearson correlation co-efficient (`pearsonr`), which is included in scipy (`scipy.stats`). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wu Plamer</td>\n",
       "      <td>0.584229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PPMI</td>\n",
       "      <td>0.357784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LSA</td>\n",
       "      <td>0.414849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>0.291747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Model  Correlation\n",
       "0  Wu Plamer     0.584229\n",
       "1       PPMI     0.357784\n",
       "2        LSA     0.414849\n",
       "3   Word2Vec     0.291747"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "import pandas as pd\n",
    "\n",
    "# transform dict datasets to array in the same order\n",
    "def get_arrays(similarity_dictionary):\n",
    "    array_gold_std=np.zeros(len(dataset_filtered_2))\n",
    "    array_similarity=np.zeros(len(similarity_dictionary))\n",
    "    index=0\n",
    "    for key in dataset_filtered_2:\n",
    "        array_gold_std[index]=dataset_filtered_2[key]\n",
    "        array_similarity[index]=similarity_dictionary[key]\n",
    "        index+=1\n",
    "    \n",
    "    return array_gold_std,array_similarity\n",
    "\n",
    "# get the coefficient \n",
    "def get_pearsonr(similarity_dictionary):   \n",
    "    gold_std, similarity=get_arrays(similarity_dictionary)  \n",
    "    r,p=pearsonr(gold_std,similarity)\n",
    "    return r\n",
    "\n",
    "        \n",
    "# create a dataframe to show results\n",
    "df_sim=pd.DataFrame(columns=['Model','Correlation'])\n",
    "df_sim=df_sim.append({'Model':'Wu Plamer','Correlation':get_pearsonr(dataset_wu_palmer)}, ignore_index=True)\n",
    "df_sim=df_sim.append({'Model':'PPMI','Correlation':get_pearsonr(dataset_ppmi)}, ignore_index=True)\n",
    "df_sim=df_sim.append({'Model':'LSA','Correlation':get_pearsonr(dataset_cosine_similarity)}, ignore_index=True)\n",
    "df_sim=df_sim.append({'Model':'Word2Vec','Correlation':get_pearsonr(dataset_word2vec_similarity)}, ignore_index=True)\n",
    "\n",
    "\n",
    "df_sim\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
